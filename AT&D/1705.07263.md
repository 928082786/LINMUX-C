# Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods

We make the following contributions:
1. We find that many defenses are unable to detect adversarial examples, even when the attacker is oblivious to the
specific defense used.
2. We break all existing detection methods in the white-box
(and black-box) setting by showing howto pick good attackerloss
functions for each defense.
3. We draw conclusions about the space of adversarial examples,
and offer a note of caution about evaluating solely
on MNIST; it appears that MNIST has somewhat different
security properties than CIFAR.
4. We provide recommendations for evaluating defenses.
---
> The author always claimed that FGSM and JSMA is too weak to evaluation whether a detection method is better.

>Why?

>Answer : We re-implement these two <font color=DeepSkyBlue >defenses </font> and find that adversarial retraining is able to detect adversarial examples when generated with the fast gradient sign and JSMA attacks with near-100% accuracy.
<font color=DeepSkyBlue >
> - *On the (Statistical) Detection of Adversarial Examples.
arXiv preprint arXiv:1702.06280 (2017).*
> - *Adversarial and Clean Data
Are Not Twins. arXiv preprint arXiv:1704.04960 (2017).*
</font>

---
## <font color=LemonChiffon>Ready</font>
### <font color=IndianRed> Three attack methods</font>
1. A strong attack (Zero-Knowledge):
2. An adaptive, white-box attack (Perfect-Knowledge)
3. A black-box attack (Limited-Knowledge):

### <font color=IndianRed> Attack method </font>
- C&W method

### <font color=IndianRed> Defence methods </font>

> - <font color=HotPink>Grosse</font> *On the (Statistical) Detection of Adversarial Examples.
arXiv preprint arXiv:1702.06280 (2017).*
> - <font color=HotPink>Gong</font> *Adversarial and Clean Data
Are Not Twins. arXiv preprint arXiv:1704.04960 (2017).*
> - <font color=HotPink>Metzen</font> *On Detecting Adversarial Perturbations. In International Conference on Learning Representations. arXiv preprint arXiv:1702.04267.*
> 

## <font color=LemonChiffon>Show</font>
### <font color=IndianRed>Zero-Knowledge</font>
Grosse and Gong methods perform well on MNIST, but not on CIFAR10. 

High false positive rate on Metzen method
### <font color=IndianRed>Perfect-Knowledge</font>
- <font color=Plum>Grosse method</font>
  
  Use C&W directly(less distortion)
- <font color=Plum> Gong method </font>
  
  Define a function 
    $$
    G(x)_{i} = \left\{
        \begin{aligned}  
        Z_{F}(x)_{i} & &i <=N \\
        (Z_{D}+1)*max_{j}Z_{F}(x)_{j} & &i=N+1        \end{aligned}
        \right.
    $$
   Now we treat this function G as if it were a neural network, and
feed it directly into C&W’s attack algorithm instead of the function
Z. It is easy to see that if an instance $x ′$ is classified by $G$ as label
$l ≤ N$ then both $C(x ′) = l$ and the detector classifies $x ′$ as natural
(non-adversarial). This approach successfully generates adversarial
examples against Gong’s defense with 100% success.
### <font color=IndianRed>Limited-Knowledge</font>

Split the training set in half. First half is known to the attacker and the second half is used by the defender to train their
network. Then apply the C&W attack to each instance in the test set to generate adversarial examples for $R_{1}$ (model trained on first half dataset), and we test whether they fool $R_{2}$ (the other half one).

> **In fact this is transfering attacking**


